{"cells":[{"cell_type":"markdown","id":"e3792687","metadata":{"id":"e3792687"},"source":["# Cleaning the dataset"]},{"cell_type":"code","execution_count":null,"id":"dcf7859e","metadata":{"id":"dcf7859e"},"outputs":[],"source":["import csv\n","\n","def extract_subreddit_names(input_csv, output_csv):\n","    with open(input_csv, 'r', newline='', encoding='utf-8') as infile:\n","        reader = csv.reader(infile)\n","        with open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n","            writer = csv.writer(outfile)\n","            for row in reader:\n","                # Write only the first column (subreddit names)\n","                writer.writerow([row[0]])\n","\n","# Usage\n","input_csv = '/Users/ElevenyCHEN/Desktop/Mod_Datasets/SubReddit-time.csv'  # Replace with the path to your existing CSV file\n","output_csv = '/Users/ElevenyCHEN/Desktop/Mod_Datasets/SubReddit-list.csv'   # Replace with the path where you want the new CSV file to be saved\n","extract_subreddit_names(input_csv, output_csv)\n"]},{"cell_type":"markdown","id":"1758a138","metadata":{"id":"1758a138"},"source":["# Counting posting users"]},{"cell_type":"code","source":["import zstandard as zstd\n","import json\n","import csv\n","import os\n","import threading\n","from datetime import datetime, timedelta\n","import time\n","\n","\n","size = 1024 * 1024 * 10\n","\n","# Generate file paths\n","def generate_file_paths(start_year, start_month, end_year, end_month):\n","    current = datetime(start_year, start_month, 1)\n","    end = datetime(end_year, end_month, 1)\n","    paths = []\n","    while current >= end:\n","        file_name = current.strftime(\"%Y-%m\") + \".zst\"\n","        file_path = f'D:\\Eleveny\\RS_{file_name}'  # Adjust the path as needed\n","        paths.append(file_path)\n","        current -= timedelta(days=1)\n","        current = current.replace(day=1)\n","    return paths\n","\n","\n","# Define a lock for thread safety\n","lock = threading.Lock()\n","\n","# Worker function to process a single .zst file\n","def process_file(file_path, subreddit_authors, time_mark):\n","    print(f\"Starting processing file: {file_path} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","    line_count = 0\n","    update_interval = 1000000  # Update the progress every 1,000,000 lines\n","\n","    with open(file_path, 'rb') as compressed:\n","        dctx = zstd.ZstdDecompressor(max_window_size=2147483648)\n","        with dctx.stream_reader(compressed) as reader:\n","            buffer = ''\n","            while True:\n","                chunk = reader.read(size).decode('utf-8', errors='ignore')\n","                buffer += chunk\n","                lines = buffer.split('\\n')\n","                buffer = lines.pop()\n","\n","                for line in lines:\n","                    if line:\n","                        line_count += 1\n","                        try:\n","                            json_obj = json.loads(line)\n","                            subreddit = json_obj.get(\"subreddit\")\n","                            author = json_obj.get(\"author\")\n","                            if subreddit and author:\n","                                with lock:\n","                                    if subreddit not in subreddit_authors:\n","                                        subreddit_authors[subreddit] = {}\n","                                    if time_mark not in subreddit_authors[subreddit]:\n","                                        subreddit_authors[subreddit][time_mark] = set()\n","                                    subreddit_authors[subreddit][time_mark].add(author)\n","                        except json.JSONDecodeError:\n","                            continue\n","\n","                        # Periodic progress update\n","                        if line_count % update_interval == 0:\n","                            print(f\"Processing file: {file_path}, lines processed: {line_count}, time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","                if not chunk:\n","                    break\n","\n","    print(f\"Finished processing file: {file_path} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","\n","\n","# Main function to process subreddits\n","def process_subreddits(subreddits, start_year, start_month, end_year, end_month, output_csv):\n","    file_paths = generate_file_paths(start_year, start_month, end_year, end_month)\n","    subreddit_authors = {subreddit: {} for subreddit in subreddits}\n","\n","    threads = []\n","    for file_path in file_paths:\n","        time_mark = file_path.split('/')[-1].split('.')[0]  # Format: YYYY-MM\n","        thread = threading.Thread(target=process_file, args=(file_path, subreddit_authors, time_mark))\n","        threads.append(thread)\n","        thread.start()\n","\n","    # Wait for all threads to complete\n","    for thread in threads:\n","        thread.join()\n","\n","    # Write the output CSV\n","    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n","        writer = csv.writer(csvfile)\n","\n","        # Writing headers\n","        headers = [\"subreddit\"] + sorted([file_path.split('/')[-1].split('.')[0] for file_path in file_paths])\n","        writer.writerow(headers)\n","\n","        # Writing data rows\n","        for subreddit, data in subreddit_authors.items():\n","            row = [subreddit] + [len(data.get(month, [])) for month in headers[1:]]\n","            writer.writerow(row)\n","\n","# Read the subreddit list and process the subreddits\n","subreddit_list_csv = r'D:\\Eleveny\\SubReddit-list.csv'  # Adjust the path as needed\n","subreddits = []\n","with open(subreddit_list_csv, 'r', newline='', encoding='utf-8') as csvfile:\n","    reader = csv.reader(csvfile)\n","    next(reader)  # Skip header\n","    for row in reader:\n","        subreddits.append(row[0])\n","\n","# Process the subreddits and write to a new CSV\n","output_csv = r'D:\\Eleveny\\subreddit_posting_users.csv'  # Adjust the path as needed\n","process_subreddits(subreddits, 2016, 5, 2016, 1, output_csv)\n"],"metadata":{"id":"fLZEXBogR12X"},"id":"fLZEXBogR12X","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Not excluding 0 version"],"metadata":{"id":"NDfab0laSDeE"},"id":"NDfab0laSDeE"},{"cell_type":"code","execution_count":null,"id":"efdfb3cb","metadata":{"id":"efdfb3cb"},"outputs":[],"source":["import csv\n","\n","def read_and_rank_subreddits(input_csv):\n","    subreddits = []\n","    with open(input_csv, 'r', newline='', encoding='utf-8') as csvfile:\n","        reader = csv.reader(csvfile)\n","        headers = next(reader)\n","\n","        for row in reader:\n","            subreddit, posting_users = row[0], int(row[1])  # Reading the 'posting_users' count\n","            subreddits.append((subreddit, posting_users))\n","\n","    # Sort subreddits based on 'posting_users' count\n","    subreddits.sort(key=lambda x: x[1], reverse=True)\n","\n","    # Calculate cumulative distribution\n","    total = sum(posting_users for _, posting_users in subreddits)\n","    cumulative = 0\n","    top_95_percent = []\n","    top_90_percent = []\n","    for subreddit, posting_users in subreddits:\n","        cumulative += posting_users\n","        percentage = cumulative / total\n","        if percentage <= 0.95:\n","            top_95_percent.append((subreddit, posting_users))\n","        if percentage <= 0.90:\n","            top_90_percent.append((subreddit, posting_users))\n","\n","    return top_95_percent, top_90_percent\n","\n","def write_to_csv(subreddits, output_csv):\n","    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow([\"subreddit\", \"posting_users\"])  # Header\n","        for subreddit, posting_users in subreddits:\n","            writer.writerow([subreddit, posting_users])\n","\n","# Usage\n","input_csv = r'D:\\Eleveny\\subreddit_posting_users.csv'  # Adjust the path as needed\n","top_95_percent, top_90_percent = read_and_rank_subreddits(input_csv)\n","\n","# Write to separate CSV files\n","output_csv_95 = r'D:\\Eleveny\\top_95_percent_subreddits.csv'\n","output_csv_90 = r'D:\\Eleveny\\top_90_percent_subreddits.csv'\n","write_to_csv(top_95_percent, output_csv_95)\n","write_to_csv(top_90_percent, output_csv_90)"]},{"cell_type":"markdown","source":["Excluding 0 version"],"metadata":{"id":"KZ8QsYlkSKCK"},"id":"KZ8QsYlkSKCK"},{"cell_type":"code","source":["import csv\n","\n","def read_and_rank_subreddits(input_csv):\n","    subreddits = []\n","    with open(input_csv, 'r', newline='', encoding='utf-8') as csvfile:\n","        reader = csv.reader(csvfile)\n","        headers = next(reader)\n","\n","        for row in reader:\n","            subreddit, posting_users = row[0], int(row[1])\n","            if posting_users > 0:  # Exclude subreddits with 0 posting users\n","                subreddits.append((subreddit, posting_users))\n","\n","    # Sort subreddits based on 'posting_users' count\n","    subreddits.sort(key=lambda x: x[1], reverse=True)\n","\n","    # Calculate cumulative distribution\n","    total = sum(posting_users for _, posting_users in subreddits if posting_users > 0)\n","    cumulative = 0\n","    top_95_percent = []\n","    top_90_percent = []\n","    for subreddit, posting_users in subreddits:\n","        cumulative += posting_users\n","        percentage = cumulative / total\n","        if percentage <= 0.95:\n","            top_95_percent.append((subreddit, posting_users))\n","        if percentage <= 0.90:\n","            top_90_percent.append((subreddit, posting_users))\n","\n","    return top_95_percent, top_90_percent\n","\n","def write_to_csv(subreddits, output_csv):\n","    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow([\"subreddit\", \"posting_users\"])  # Header\n","        for subreddit, posting_users in subreddits:\n","            writer.writerow([subreddit, posting_users])\n","\n","# Usage\n","input_csv = r'D:\\Eleveny\\subreddit_posting_users.csv'  # Adjust the path as needed\n","top_95_percent, top_90_percent = read_and_rank_subreddits(input_csv)\n","\n","# Write to separate CSV files\n","output_csv_95 = r'D:\\Eleveny\\top_95_percent_subreddits.csv'\n","output_csv_90 = r'D:\\Eleveny\\top_90_percent_subreddits.csv'\n","write_to_csv(top_95_percent, output_csv_95)\n","write_to_csv(top_90_percent, output_csv_90)"],"metadata":{"id":"Opnbz1kCSLpT"},"id":"Opnbz1kCSLpT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting"],"metadata":{"id":"dNuUlH3kSMqd"},"id":"dNuUlH3kSMqd"},{"cell_type":"code","source":["!pip install matplotlib"],"metadata":{"id":"uK0uJAOoRdzT"},"id":"uK0uJAOoRdzT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","import matplotlib.pyplot as plt\n","\n","def read_and_rank_subreddits(input_csv):\n","    subreddits = []\n","    with open(input_csv, 'r', newline='', encoding='utf-8') as csvfile:\n","        reader = csv.reader(csvfile)\n","        headers = next(reader)\n","\n","        for row in reader:\n","            subreddit, posting_users = row[0], int(row[1])\n","            if posting_users > 0:  # Exclude subreddits with 0 posting users\n","                subreddits.append((subreddit, posting_users))\n","\n","    subreddits.sort(key=lambda x: x[1], reverse=True)\n","    return subreddits\n","\n","def plot_distribution(subreddits):\n","    # Extract posting user counts\n","    posting_users = [count for _, count in subreddits]\n","\n","    # Create a plot\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(range(len(posting_users)), posting_users, color='blue')\n","    plt.xlabel('Subreddits')\n","    plt.ylabel('Posting Users Count')\n","    plt.title('Distribution of Posting Users per Subreddit')\n","    plt.show()\n","\n","# Usage\n","input_csv = r'D:\\Eleveny\\subreddit_posting_users.csv'  # Adjust the path as needed\n","subreddits = read_and_rank_subreddits(input_csv)\n","\n","# Print the number of subreddits and plot distribution\n","print(f\"Total number of subreddits: {len(subreddits)}\")\n","plot_distribution(subreddits)"],"metadata":{"id":"HxwjNM6WRfrQ"},"id":"HxwjNM6WRfrQ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}