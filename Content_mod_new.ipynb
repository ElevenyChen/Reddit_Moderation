{"cells":[{"cell_type":"code","execution_count":null,"id":"b9935e46","metadata":{"id":"b9935e46","outputId":"d24e3fd0-b6ec-4cda-acd1-e26c203a1848"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: zstandard in /Users/ElevenyCHEN/anaconda3/lib/python3.11/site-packages (0.19.0)\r\n"]}],"source":["# Library\n","!pip install zstandard"]},{"cell_type":"code","execution_count":null,"id":"03fd755e","metadata":{"scrolled":true,"id":"03fd755e","outputId":"70e729af-212d-4106-c1a3-1c97a114062c"},"outputs":[{"data":{"text/plain":["{'total': 8589934592,\n"," 'available': 2644443136,\n"," 'percent': 69.2,\n"," 'used': 3992797184,\n"," 'free': 110444544,\n"," 'active': 2501214208,\n"," 'inactive': 2498052096,\n"," 'wired': 1491582976}"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import psutil\n","\n","# Check Memory\n","dict(psutil.virtual_memory()._asdict())"]},{"cell_type":"markdown","id":"66f2fff2","metadata":{"id":"66f2fff2"},"source":["# Subreddit and Post Time"]},{"cell_type":"code","execution_count":null,"id":"20d99480","metadata":{"id":"20d99480","outputId":"f591589a-25f7-4758-fd0f-1dafa89fc3db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique subreddit ids: 742331\n"]}],"source":["import zstandard as zstd\n","import json\n","\n","size = 1024 * 1024 * 1\n","\n","def count_unique_subreddit_ids(file_path, chunk_size=size):\n","    unique_ids = set()\n","    with open(file_path, 'rb') as compressed:\n","        dctx = zstd.ZstdDecompressor(max_window_size=2147483648)\n","        with dctx.stream_reader(compressed) as reader:\n","            buffer = ''\n","            while True:\n","                chunk = reader.read(chunk_size).decode('utf-8', errors='ignore')\n","                buffer += chunk\n","                lines = buffer.split('\\n')\n","                buffer = lines.pop()  # Save incomplete line for next chunk\n","\n","                for line in lines:\n","                    if line:\n","                        try:\n","                            json_obj = json.loads(line)\n","                            subreddit_id = json_obj.get(\"subreddit_id\", None)\n","                            if subreddit_id:\n","                                unique_ids.add(subreddit_id)\n","                        except json.JSONDecodeError:\n","                            continue  # Skip invalid JSON lines\n","\n","                if not chunk:\n","                    break\n","\n","    return len(unique_ids)\n","\n","file_path = r'D:\\Eleveny\\RS_2023-03.zst'\n","count = count_unique_subreddit_ids(file_path)\n","print(f\"Unique subreddit ids: {count}\")\n","\n","# Unique subreddit ids: 742331 (RS_2023-03)"]},{"cell_type":"markdown","source":["# Initializing dataset"],"metadata":{"id":"wSIaM53Pk6sA"},"id":"wSIaM53Pk6sA"},{"cell_type":"code","execution_count":null,"id":"97b7f970","metadata":{"id":"97b7f970"},"outputs":[],"source":["import zstandard as zstd\n","import json\n","import csv\n","\n","size = 1024 * 1024 * 1\n","\n","def decompress_and_process(file_path, output_csv):\n","    unique_subreddits = set()\n","\n","    # Decompress the file\n","    with open(file_path, 'rb') as compressed:\n","        dctx = zstd.ZstdDecompressor(max_window_size=2147483648)\n","        with dctx.stream_reader(compressed) as reader:\n","            buffer = ''\n","            while True:\n","                chunk = reader.read(size).decode('utf-8', errors='ignore')\n","                buffer += chunk\n","                lines = buffer.split('\\n')\n","                buffer = lines.pop()\n","\n","                for line in lines:\n","                    if line:\n","                        try:\n","                            json_obj = json.loads(line)\n","                            subreddit = json_obj.get(\"subreddit\", None)\n","                            if subreddit:\n","                                unique_subreddits.add(subreddit)\n","                        except json.JSONDecodeError:\n","                            continue\n","\n","                if not chunk:\n","                    break\n","\n","    # Write to CSV\n","    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow([\"subreddit\", \"2016-05\"])  # Header row\n","        for subreddit in unique_subreddits:\n","            writer.writerow([subreddit, 1])\n","\n","\n","file_path = r'/Users/ElevenyCHEN/Desktop/Mod_Datasets/RS_2016-05.zst'\n","output_csv = r'/Users/ElevenyCHEN/Desktop/Mod_Datasets/SubReddit-time.csv'\n","decompress_and_process(file_path, output_csv)"]},{"cell_type":"markdown","source":["## Single-core CPU Version"],"metadata":{"id":"l5eOInzJpZ9B"},"id":"l5eOInzJpZ9B"},{"cell_type":"code","execution_count":null,"id":"f2c1ec55","metadata":{"id":"f2c1ec55"},"outputs":[],"source":["import zstandard as zstd\n","import json\n","import csv\n","from datetime import datetime, timedelta\n","\n","size = 1024 * 1024 * 1\n","\n","\n","def generate_file_paths(start_year, start_month, end_year, end_month):\n","    current = datetime(start_year, start_month, 1)\n","    end = datetime(end_year, end_month, 1)\n","    paths = []\n","    while current >= end:\n","        file_name = current.strftime(\"%Y-%m\") + \".zst\"\n","        file_path = f'/Users/ElevenyCHEN/Desktop/Mod_Datasets/RS_{file_name}'\n","        paths.append(file_path)\n","        current -= timedelta(days=1)\n","        current = current.replace(day=1)\n","    return paths\n","\n","def decompress_and_update(file_paths, existing_csv):\n","    # Read the existing CSV\n","    existing_data = {}\n","    processed_months = set()\n","    with open(existing_csv, 'r', newline='', encoding='utf-8') as csvfile:\n","        reader = csv.reader(csvfile)\n","        headers = next(reader)\n","        processed_months.update(headers[1:])  # Exclude the first header ('subreddit')\n","        for row in reader:\n","            existing_data[row[0]] = row[1:]\n","\n","    for file_path in file_paths:\n","        new_month = file_path.split('/')[-1].split('.')[0]  # Extract month-year from file path\n","\n","        # Skip already processed months\n","        if new_month in processed_months:\n","            continue\n","\n","        unique_subreddits = set()\n","\n","        # Decompress and process the .zst file\n","        with open(file_path, 'rb') as compressed:\n","            dctx = zstd.ZstdDecompressor(max_window_size=2147483648)\n","            with dctx.stream_reader(compressed) as reader:\n","                buffer = ''\n","                while True:\n","                    chunk = reader.read(size).decode('utf-8', errors='ignore')\n","                    buffer += chunk\n","                    lines = buffer.split('\\n')\n","                    buffer = lines.pop()\n","\n","                    for line in lines:\n","                        if line:\n","                            try:\n","                                json_obj = json.loads(line)\n","                                subreddit = json_obj.get(\"subreddit\", None)\n","                                if subreddit:\n","                                    unique_subreddits.add(subreddit)\n","                            except json.JSONDecodeError:\n","                                continue\n","\n","                    if not chunk:\n","                        break\n","\n","        # Update CSV data\n","        headers.append(new_month)\n","        for subreddit in unique_subreddits:\n","            if subreddit not in existing_data:\n","                # Initialize new subreddit data\n","                existing_data[subreddit] = ['0'] * (len(headers) - 2)\n","            existing_data[subreddit].append('1')\n","\n","    # Write the updated CSV\n","    with open(existing_csv, 'w', newline='', encoding='utf-8') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow([\"subreddit\"] + headers[1:])\n","        for subreddit, months in existing_data.items():\n","            # Ensure each row has the correct length\n","            if len(months) < len(headers) - 1:\n","                months.extend(['0'] * (len(headers) - 1 - len(months)))\n","            writer.writerow([subreddit] + months)\n","\n","\n","# Example usage\n","file_paths = generate_file_paths(2016, 5, 2016, 1)\n","existing_csv = r'/Users/ElevenyCHEN/Desktop/Mod_Datasets/SubReddit-time.csv'\n","decompress_and_update(file_paths, existing_csv)\n"]},{"cell_type":"markdown","source":["## Multireading iteration in subreddit existing time"],"metadata":{"id":"ZGOAp72Cli79"},"id":"ZGOAp72Cli79"},{"cell_type":"code","source":["import zstandard as zstd\n","import json\n","import csv\n","import threading\n","from datetime import datetime, timedelta\n","\n","size = 1024 * 1024 * 1\n","\n","# Generate file paths\n","def generate_file_paths(start_year, start_month, end_year, end_month):\n","    current = datetime(start_year, start_month, 1)\n","    end = datetime(end_year, end_month, 1)\n","    paths = []\n","    while current >= end:\n","        file_name = current.strftime(\"%Y-%m\") + \".zst\"\n","        file_path = f'/Users/ElevenyCHEN/Desktop/Mod_Datasets/RS_{file_name}'\n","        paths.append(file_path)\n","        current -= timedelta(days=1)\n","        current = current.replace(day=1)\n","    return paths\n","\n","# Worker function to process a single .zst file\n","def process_file(file_path, existing_data, headers, processed_months, lock):\n","    print(f\"Starting processing file: {file_path} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","    line_count = 0\n","    update_interval = 1000000  # Update the progress every 1,000,000 lines\n","\n","    new_month = file_path.split('/')[-1].split('.')[0]\n","\n","    if new_month not in processed_months:\n","        unique_subreddits = set()\n","\n","        with open(file_path, 'rb') as compressed:\n","            dctx = zstd.ZstdDecompressor(max_window_size=2147483648)\n","            with dctx.stream_reader(compressed) as reader:\n","                buffer = ''\n","                while True:\n","                    chunk = reader.read(size).decode('utf-8', errors='ignore')\n","                    buffer += chunk\n","                    lines = buffer.split('\\n')\n","                    buffer = lines.pop()\n","\n","                    for line in lines:\n","                        if line:\n","                            line_count += 1\n","                            try:\n","                                json_obj = json.loads(line)\n","                                subreddit = json_obj.get(\"subreddit\", None)\n","                                if subreddit:\n","                                    unique_subreddits.add(subreddit)\n","                            except json.JSONDecodeError:\n","                                continue\n","\n","                    if not chunk:\n","                        break\n","\n","        print(f\"Finished processing file: {file_path} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","        with lock:\n","            headers.add(new_month)\n","            for subreddit in unique_subreddits:\n","                if subreddit not in existing_data:\n","                    existing_data[subreddit] = ['0'] * (len(headers) - 2)\n","                existing_data[subreddit].append('1')\n","\n","# Main function to decompress and update\n","def decompress_and_update(file_paths, existing_csv):\n","    existing_data = {}\n","    headers = set([\"subreddit\"])\n","    processed_months = set()\n","    lock = threading.Lock()\n","\n","    # Read existing CSV\n","    with open(existing_csv, 'r', newline='', encoding='utf-8') as csvfile:\n","        reader = csv.reader(csvfile)\n","        first_row = next(reader)\n","        headers.update(first_row[1:])\n","        processed_months.update(first_row[1:])\n","        for row in reader:\n","            existing_data[row[0]] = row[1:]\n","\n","    threads = []\n","    for file_path in file_paths:\n","        thread = threading.Thread(target=process_file, args=(file_path, existing_data, headers, processed_months, lock))\n","        threads.append(thread)\n","        thread.start()\n","\n","    # Progress reporting and wait for all threads to complete\n","    for thread in threads:\n","        thread.join()\n","        print(f\"Finished processing file: {file_path}\")\n","\n","    # Write the updated CSV\n","    with open(existing_csv, 'w', newline='', encoding='utf-8') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow(sorted(list(headers)))\n","        for subreddit, months in existing_data.items():\n","            writer.writerow([subreddit] + months)\n","\n","# Example usage\n","file_paths = generate_file_paths(2016, 5, 2016, 1)\n","existing_csv = r'/Users/ElevenyCHEN/Desktop/Mod_Datasets/SubReddit-time.csv'\n","decompress_and_update(file_paths, existing_csv)"],"metadata":{"id":"6PbzGAonlyXG"},"id":"6PbzGAonlyXG","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Filling blank cells"],"metadata":{"id":"txBrKkDulemX"},"id":"txBrKkDulemX"},{"cell_type":"code","execution_count":null,"id":"c065f3a8","metadata":{"id":"c065f3a8"},"outputs":[],"source":["def fill_empty_cells_in_csv(csv_file):\n","    updated_data = []\n","    with open(csv_file, 'r', newline='', encoding='utf-8') as csvfile:\n","        reader = csv.reader(csvfile)\n","        headers = next(reader)  # Keep headers\n","        updated_data.append(headers)\n","\n","        for row in reader:\n","            # Ensure the row has the correct number of columns\n","            if len(row) < len(headers):\n","                row.extend(['0'] * (len(headers) - len(row)))\n","\n","            # Replace None or empty/whitespace-only cells with '0'\n","            updated_row = ['0' if cell is None or cell.strip() == '' else cell for cell in row]\n","            updated_data.append(updated_row)\n","8\n","    # Write the updated data back to the CSV\n","    with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerows(updated_data)\n","\n","# Example usage\n","csv_file = r'D:\\Eleveny\\SubReddit-time.csv'\n","fill_empty_cells_in_csv(csv_file)"]},{"cell_type":"markdown","id":"9d654786","metadata":{"id":"9d654786"},"source":["# Getting Subreddit and Deletion"]},{"cell_type":"markdown","source":["## Single-core CPU Version"],"metadata":{"id":"R_1grfyBpKFK"},"id":"R_1grfyBpKFK"},{"cell_type":"code","execution_count":null,"id":"7b550841","metadata":{"id":"7b550841"},"outputs":[],"source":["import zstandard as zstd\n","import json\n","import csv\n","from datetime import datetime, timedelta\n","import os\n","\n","size = 1024 * 1024 * 1\n","\n","def decompress_and_update(file_paths, input_csv, output_csv):\n","    # Read the existing data from output CSV\n","    existing_data = {}\n","    existing_headers = []\n","    if os.path.exists(output_csv):\n","        with open(output_csv, 'r', newline='', encoding='utf-8') as csvfile:\n","            reader = csv.reader(csvfile)\n","            existing_headers = next(reader)\n","            for row in reader:\n","                existing_data[row[0]] = row[1:]\n","\n","    # Fill missing data with blanks if new subreddits are found\n","    subreddits = set(existing_data.keys())\n","\n","    for file_path in file_paths:\n","        time_mark = os.path.basename(file_path).split('.')[0].split('_')[1]\n","        header_prefixes = [\"total_posts_\", \"deleted_posts_\", \"proportion_\", \"mod_delete_\", \"admin_delete_\"]\n","        new_headers = [prefix + time_mark for prefix in header_prefixes]\n","\n","        # Skip if these headers already exist (data already processed)\n","        if all(header in existing_headers for header in new_headers):\n","            continue\n","\n","        subreddit_metrics = {subreddit: {'total_posts': 0, 'deleted_posts': 0, 'mod_delete': 0, 'admin_delete': 0} for subreddit in subreddits}\n","\n","        # Decompress and process the .zst file\n","        with open(file_path, 'rb') as compressed:\n","            dctx = zstd.ZstdDecompressor(max_window_size=2147483648)\n","            with dctx.stream_reader(compressed) as reader:\n","                buffer = ''\n","                while True:\n","                    chunk = reader.read(size).decode('utf-8', errors='ignore')\n","                    buffer += chunk\n","                    lines = buffer.split('\\n')\n","                    buffer = lines.pop()\n","\n","                    for line in lines:\n","                        if line:\n","                            try:\n","                                json_obj = json.loads(line)\n","                                subreddit = json_obj.get(\"subreddit\", None)\n","                                if subreddit and subreddit in subreddit_metrics:\n","                                    metrics = subreddit_metrics[subreddit]\n","                                    metrics['total_posts'] += 1\n","                                    distinguished = json_obj.get(\"distinguished\", None)\n","                                    if distinguished == \"moderator\":\n","                                        metrics['deleted_posts'] += 1\n","                                        metrics['mod_delete'] += 1\n","                                    elif distinguished == \"admin\":\n","                                        metrics['deleted_posts'] += 1\n","                                        metrics['admin_delete'] += 1\n","                            except json.JSONDecodeError:\n","                                continue\n","\n","                    if not chunk:\n","                        break\n","\n","        # Update existing_data with new metrics or leave blank\n","        for subreddit in subreddits:\n","            metrics = subreddit_metrics.get(subreddit)\n","            if metrics:\n","                proportion = metrics['deleted_posts'] / metrics['total_posts'] if metrics['total_posts'] > 0 else 0\n","                new_data = [metrics['total_posts'], metrics['deleted_posts'], proportion, metrics['mod_delete'], metrics['admin_delete']]\n","            else:\n","                new_data = ['', '', '', '', '']\n","\n","            existing_data[subreddit].extend(new_data)\n","\n","        # Update headers\n","        existing_headers.extend(new_headers)\n","\n","    # Write the updated data to the output CSV\n","    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow([\"subreddit\"] + existing_headers[1:])\n","        for subreddit, data in existing_data.items():\n","            writer.writerow([subreddit] + data)\n","\n","# Example usage\n","file_paths = generate_file_paths(2016, 5, 2022, 9)\n","input_csv = r'D:\\Eleveny\\SubReddit-time.csv'\n","output_csv = r'D:\\Eleveny\\SubReddit-deletion.csv'\n","decompress_and_update(file_paths, input_csv, output_csv)\n"]},{"cell_type":"markdown","source":["## Multireading Version"],"metadata":{"id":"Vy2G346HpEHM"},"id":"Vy2G346HpEHM"},{"cell_type":"code","source":["import zstandard as zstd\n","import json\n","import csv\n","from datetime import datetime, timedelta\n","import os\n","import threading\n","\n","size = 1024 * 1024 * 1\n","\n","# Generate file paths\n","def generate_file_paths(start_year, start_month, end_year, end_month):\n","    current = datetime(start_year, start_month, 1)\n","    end = datetime(end_year, end_month, 1)\n","    paths = []\n","    while current >= end:\n","        file_name = current.strftime(\"%Y-%m\") + \".zst\"\n","        file_path = f'/Users/ElevenyCHEN/Desktop/Mod_Datasets/RS_{file_name}'\n","        paths.append(file_path)\n","        current -= timedelta(days=1)\n","        current = current.replace(day=1)\n","    return paths\n","\n","\n","# Worker function to process a single .zst file\n","def process_file(file_path, existing_data, subreddit_metrics, headers, lock):\n","    print(f\"Starting processing file: {file_path} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","    time_mark = os.path.basename(file_path).split('.')[0].split('_')[1]\n","    header_prefixes = [\"total_posts_\", \"deleted_posts_\", \"proportion_\", \"mod_delete_\", \"admin_delete_\"]\n","    new_headers = [prefix + time_mark for prefix in header_prefixes]\n","\n","    line_count = 0\n","    update_interval = 1000000  # Update the progress every 1,000,000 lines\n","\n","    with open(file_path, 'rb') as compressed:\n","        dctx = zstd.ZstdDecompressor(max_window_size=2147483648)\n","        with dctx.stream_reader(compressed) as reader:\n","            buffer = ''\n","            while True:\n","                chunk = reader.read(size).decode('utf-8', errors='ignore')\n","                buffer += chunk\n","                lines = buffer.split('\\n')\n","                buffer = lines.pop()\n","\n","                for line in lines:\n","                    if line:\n","                        line_count += 1\n","                        try:\n","                            json_obj = json.loads(line)\n","                            subreddit = json_obj.get(\"subreddit\", None)\n","                            if subreddit and subreddit in subreddit_metrics:\n","                                metrics = subreddit_metrics[subreddit]\n","                                metrics['total_posts'] += 1\n","                                distinguished = json_obj.get(\"distinguished\", None)\n","                                if distinguished == \"moderator\":\n","                                    metrics['deleted_posts'] += 1\n","                                    metrics['mod_delete'] += 1\n","                                elif distinguished == \"admin\":\n","                                    metrics['deleted_posts'] += 1\n","                                    metrics['admin_delete'] += 1\n","                        except json.JSONDecodeError:\n","                            continue\n","\n","                        # Periodic progress update\n","                        if line_count % update_interval == 0:\n","                            print(f\"Processing file: {file_path}, lines processed: {line_count}, time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","                if not chunk:\n","                    break\n","\n","    print(f\"Finished processing file: {file_path} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","    with lock:\n","        # Update existing_data with new metrics or leave blank\n","        for subreddit, metrics in subreddit_metrics.items():\n","            proportion = metrics['deleted_posts'] / metrics['total_posts'] if metrics['total_posts'] > 0 else 0\n","            new_data = [metrics['total_posts'], metrics['deleted_posts'], proportion, metrics['mod_delete'], metrics['admin_delete']]\n","            existing_data[subreddit].extend(new_data)\n","        headers.update(new_headers)\n","\n","# Main function to decompress and update\n","def decompress_and_update(file_paths, input_csv, output_csv):\n","    existing_data = {}\n","    headers = set([\"subreddit\"])\n","    lock = threading.Lock()\n","\n","    # Read the existing data from output CSV\n","    existing_data = {}\n","    existing_headers = []\n","    if os.path.exists(output_csv):\n","        with open(output_csv, 'r', newline='', encoding='utf-8') as csvfile:\n","            reader = csv.reader(csvfile)\n","            existing_headers = next(reader)\n","            for row in reader:\n","                existing_data[row[0]] = row[1:]\n","\n","    # Fill missing data with blanks if new subreddits are found\n","    subreddits = set(existing_data.keys())\n","\n","    for file_path in file_paths:\n","        time_mark = os.path.basename(file_path).split('.')[0].split('_')[1]\n","        header_prefixes = [\"total_posts_\", \"deleted_posts_\", \"proportion_\", \"mod_delete_\", \"admin_delete_\"]\n","        new_headers = [prefix + time_mark for prefix in header_prefixes]\n","\n","        # Skip if these headers already exist (data already processed)\n","        if all(header in existing_headers for header in new_headers):\n","            continue\n","\n","    # Initialize subreddit_metrics\n","    subreddit_metrics = {subreddit: {'total_posts': 0, 'deleted_posts': 0, 'mod_delete': 0, 'admin_delete': 0} for subreddit in existing_data.keys()}\n","\n","    threads = []\n","    for file_path in file_paths:\n","        thread = threading.Thread(target=process_file, args=(file_path, existing_data, subreddit_metrics, headers, lock))\n","        threads.append(thread)\n","        thread.start()\n","\n","    # Progress reporting and wait for all threads to complete\n","    for thread in threads:\n","        thread.join()\n","        print(f\"Finished processing file: {file_path} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","    # Write the updated data to the output CSV\n","    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow([\"subreddit\"] + sorted(list(headers)))\n","        for subreddit, data in existing_data.items():\n","            writer.writerow([subreddit] + data)\n","\n","# Example usage\n","file_paths = generate_file_paths(2016, 4, 2016, 1)\n","input_csv = r'/Users/ElevenyCHEN/Desktop/Mod_Datasets/SubReddit-time.csv'\n","output_csv = r'/Users/ElevenyCHEN/Desktop/Mod_Datasets/SubReddit-deletion.csv'\n","decompress_and_update(file_paths, input_csv, output_csv)"],"metadata":{"id":"q81Dc6Gnm3SX"},"id":"q81Dc6Gnm3SX","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}